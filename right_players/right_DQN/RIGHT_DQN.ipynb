{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'll train a DQN agent to play the game of Pong. The agent will be trained using Gymnasium library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from gymnasium) (1.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from gymnasium) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: supersuit in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from supersuit) (1.26.2)\n",
      "Requirement already satisfied: gymnasium>=0.28.1 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from supersuit) (0.29.1)\n",
      "Requirement already satisfied: tinyscaler>=1.2.6 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from supersuit) (1.2.7)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from gymnasium>=0.28.1->supersuit) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from gymnasium>=0.28.1->supersuit) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n",
      "Requirement already satisfied: torch in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: autorom[accept-rom-license] in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (0.6.1)\n",
      "Requirement already satisfied: click in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from autorom[accept-rom-license]) (8.1.7)\n",
      "Requirement already satisfied: requests in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from autorom[accept-rom-license]) (2.31.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from autorom[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ndelafuente/miniconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "!pip install gymnasium\n",
    "!pip install supersuit\n",
    "!pip install torch\n",
    "!pip install autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import supersuit as ss\n",
    "import collections\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create the environment and see what it looks like before we start applying the wrappers from supersuit library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment's observation space: Box(0, 255, (210, 160, 3), uint8)\n",
      "environment's action space: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\")\n",
    "print(\"environment's observation space:\", env.observation_space)\n",
    "print(\"environment's action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply the wrappers from supersuit library. The wrappers will do the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first wrapper is a color reduction wrapper that will make the image grayscale. This will save a lot of computation while we won't lose any relevant information as the game is pretty simple and the color of the ball and the paddles is the same and does not change during the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment's observation space: Box(0, 255, (210, 160), uint8)\n"
     ]
    }
   ],
   "source": [
    "env = ss.color_reduction_v0(env, mode=\"full\")\n",
    "\n",
    "print(\"environment's observation space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second wrapper is a resize wrapper that will resize the image to 84x84. This will also save a lot of computation and will not affect the performance of the agent as the agent will be able to see both the paddle and the ball even after the resize/cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment's observation space: Box(0, 255, (84, 84), uint8)\n"
     ]
    }
   ],
   "source": [
    "env = ss.resize_v1(env, x_size=84, y_size=84) # Resize the observation space to 84x84 \n",
    "\n",
    "print(\"environment's observation space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third wrapper is a frame stacking wrapper that will stack 4 frames together. This will allow the agent to see the movement of the ball and the paddle. This is important as the agent will be able to see the direction of the ball and the paddle and also the speed of the ball, which is important for the agent to learn how to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment's observation space: Box(0, 255, (84, 84, 4), uint8)\n"
     ]
    }
   ],
   "source": [
    "env = ss.frame_stack_v1(env, 4) # Stack 4 frames together\n",
    "\n",
    "print(\"environment's observation space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth wrapper is a dtype wrapper that will convert the data type of the image from uint8 to float32. This is important as the neural network will be able to learn faster if the data type is float32 as it is a more precise data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment's observation space: Box(0.0, 255.0, (84, 84, 4), float32)\n"
     ]
    }
   ],
   "source": [
    "env = ss.dtype_v0(env, dtype=np.float32) # Convert observations to float32\n",
    "\n",
    "print(\"environment's observation space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifth and last wrapper is a normalization wrapper that will normalize the image between 0 and 1. This is important as the neural network will be able to learn faster if the data is normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment's observation space: Box(0.0, 1.0, (84, 84, 4), float32)\n"
     ]
    }
   ],
   "source": [
    "env = ss.normalize_obs_v0(env, env_min=0, env_max=1) # Normalize observations to [0, 1]\n",
    "\n",
    "print(\"environment's observation space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our environment ready for training. Lets show how the environment looks like after starting the modelling and training phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment's observation space: Box(0.0, 1.0, (84, 84, 4), float32)\n",
      "environment's observation space shape: (84, 84, 4)\n",
      "environment's action space: Discrete(6)\n",
      "Meaning of the actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "print(\"environment's observation space:\", env.observation_space)\n",
    "print(\"environment's observation space shape:\", env.observation_space.shape)\n",
    "print(\"environment's action space:\", env.action_space)\n",
    "print(\"Meaning of the actions: \",env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll wrap them all together in a function that will create the environment and apply the wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym.make(\"ALE/Pong-v5\")\n",
    "    env = ss.color_reduction_v0(env, mode=\"full\")\n",
    "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "    env = ss.frame_stack_v1(env, 4)\n",
    "    env = ss.dtype_v0(env, dtype=np.float32)\n",
    "    env = ss.normalize_obs_v0(env, env_min=0, env_max=1)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check that our GPU's are ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 27 10:23:59 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   0  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 27%   36C    P8               4W / 250W |    327MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 27%   28C    P8               1W / 250W |      3MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   28C    P8               1W / 250W |      3MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 27%   25C    P8               1W / 250W |      3MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 27%   27C    P8               1W / 250W |      3MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:41:00.0 Off |                  N/A |\n",
      "| 27%   28C    P8               1W / 250W |      3MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2681422      C   ...e/ndelafuente/miniconda3/bin/python      324MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the neural network architecture that will serve as a function approximator for the Q function. The neural network will be a convolutional neural network. The input will be the wrapped environment and the output would be a vector of size 6, where each element in the vector represents the Q value of a specific action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the cuda device object\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see how the architecture looks like:\n",
      " \n",
      " Sequential(\n",
      "  (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (5): Flatten(start_dim=1, end_dim=-1)\n",
      "  (6): Linear(in_features=3136, out_features=512, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=512, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def DQN(obs_shape, num_actions):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(obs_shape[2], 32, kernel_size=8, stride=4),  # Use obs_shape[2] for the number of channels\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64 * 7 * 7, 512),  # Ensure that the input features to this linear layer match the output from the last conv layer\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, num_actions)\n",
    "    )\n",
    "\n",
    "\n",
    "check_env = make_env()\n",
    "check_net = DQN(check_env.observation_space.shape, check_env.action_space.n).to(device)\n",
    "print(\"Let's see how the architecture looks like:\\n \\n\", check_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experienvce Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to define the experience replay buffer.The experience replay buffer will store the experiences of the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the buffer is: 0\n",
      "Let's add some experience to the buffer\n",
      "The length of the buffer is: 1\n",
      "Let's add some more experience to the buffer\n",
      "The length of the buffer is: 2\n",
      "Let's sample the batch from the buffer\n",
      "The states are: [1 1] of type: <class 'numpy.ndarray'>\n",
      "The actions are: [2 2] of type: <class 'numpy.ndarray'>\n",
      "The rewards are: [3. 3.] of type: <class 'numpy.ndarray'>\n",
      "The terminated are: [4. 4.] of type: <class 'numpy.ndarray'>\n",
      "The truncated are: [5. 5.] of type: <class 'numpy.ndarray'>\n",
      "The new_states are: [6 6] of type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Define the experience tuple to store the experience which is composed of the state, action, terminated, truncated, truncated and new_state\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'terminated', 'truncated', 'new_state'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        #Initialize the buffer with the capacity\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        #Return the length of the buffer\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        #Append the experience to the buffer\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        #Sample the batch from the buffer\n",
    "        \n",
    "        #Choose the random indices from the buffer to be sampled\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        #Get the states, actions, rewards, terminated, truncated, new_states from the buffer by using the indices,\n",
    "        # the zip(*[]) is used to unzip the list of tuples into the list of lists \n",
    "        states, actions, rewards, terminated, truncated, new_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        \n",
    "        #Return the states, actions, rewards, terminated, truncated, new_states as numpy arrays\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), np .array(terminated, dtype=np.float32), np.array(truncated, dtype=np.float32), np.array(new_states)\n",
    "    \n",
    "\n",
    "\n",
    "check_rep = ExperienceReplay(capacity = 10000)\n",
    "print(\"The length of the buffer is:\", len(check_rep))\n",
    "print(\"Let's add some experience to the buffer\")\n",
    "check_rep.append(Experience(1,2,3,4,5,6))\n",
    "print(\"The length of the buffer is:\", len(check_rep))\n",
    "print(\"Let's add some more experience to the buffer\")\n",
    "check_rep.append(Experience(1,2,3,4,5,6))\n",
    "print(\"The length of the buffer is:\", len(check_rep))\n",
    "print(\"Let's sample the batch from the buffer\")\n",
    "states, actions, rewards, terminated, truncated, new_states = check_rep.sample(batch_size=2)\n",
    "print(\"The states are:\", states, \"of type:\", type(states))\n",
    "print(\"The actions are:\", actions, \"of type:\", type(actions))\n",
    "print(\"The rewards are:\", rewards, \"of type:\", type(rewards))\n",
    "print(\"The terminated are:\", terminated, \"of type:\", type(terminated))\n",
    "print(\"The truncated are:\", truncated, \"of type:\", type(truncated))\n",
    "print(\"The new_states are:\", new_states, \"of type:\", type(new_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Epsilon Greedy Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's define the epsilon greedy policy that will be used during the training phase. The epsilon greedy policy will be used to select the action that the agent will take. The epsilon greedy policy will select a random action with probability epsilon and will select the action with the highest Q value with probability 1-epsilon. This epsilon will be decayed over time to make the agent explore less and less as the agent learns more and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The exploration rate at step 0 is: 1.0\n",
      "The exploration rate at step 1 is: 0.999990200049\n",
      "The exploration rate at step 10 is: 0.9999020048998368\n",
      "The exploration rate at step 100 is: 0.9990204898367077\n",
      "The exploration rate at step 10000 is: 0.9067406696752404\n",
      "The exploration rate at step 100000 is: 0.38052185234801356\n",
      "The exploration rate at step 1000000 is: 0.020044491931167235\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class EpsilonGreedyStrategy:\n",
    "    def __init__(self, start, end, decay):\n",
    "        #Initialize the start, end, decay values\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "    \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        #Return the exploration rate\n",
    "        return self.end + (self.start - self.end) * np.exp(-1. * current_step / self.decay)\n",
    "    \n",
    "\n",
    "check_strat = EpsilonGreedyStrategy(1.0, 0.02, 100000)\n",
    "print(\"The exploration rate at step 0 is:\", check_strat.get_exploration_rate(0))\n",
    "print(\"The exploration rate at step 1 is:\", check_strat.get_exploration_rate(1))\n",
    "print(\"The exploration rate at step 10 is:\", check_strat.get_exploration_rate(10))\n",
    "print(\"The exploration rate at step 100 is:\", check_strat.get_exploration_rate(100))\n",
    "print(\"The exploration rate at step 10000 is:\", check_strat.get_exploration_rate(10000))\n",
    "print(\"The exploration rate at step 100000 is:\", check_strat.get_exploration_rate(100000))\n",
    "print(\"The exploration rate at step 1000000 is:\", check_strat.get_exploration_rate(1000000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define the agent. The agent will make use of the neural network, the experience replay buffer and the epsilon greedy policy that we defined earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_replay_buffer):\n",
    "        self.env = env\n",
    "        self.exp_replay_buffer = exp_replay_buffer\n",
    "        self._reset()\n",
    "    \n",
    "    def _reset(self):\n",
    "        #Reset the environment\n",
    "        self.current_state, info = self.env.reset() # Get the current state and info from the environment\n",
    "        self.total_reward = 0.0\n",
    "        self.current_episode_steps = 0\n",
    "    \n",
    "    def step(self, dqn, strategy, device=device):\n",
    "        #set the done_reward to None initially, this is the value which will be returned when the episode is done\n",
    "        done_reward = None\n",
    "        \n",
    "        #Choose the action based on the strategy\n",
    "        exploration_rate = strategy.get_exploration_rate(self.current_episode_steps)\n",
    "        \n",
    "        if not np.random.rand() < exploration_rate:\n",
    "            action = self.env.action_space.sample()\n",
    "            #print(\"The action is:\", action)\n",
    "        \n",
    "        else:\n",
    "            state = torch.tensor(np.array(self.current_state).transpose(2, 0, 1), dtype=torch.float32).to(device)# Convert the current state to a tensor of shape [ 4, 84, 84]\n",
    "            #print(\"The state shape is:\", state.shape)\n",
    "            q_values = dqn(state.unsqueeze(0))\n",
    "            #print(\"The q_values shape is:\", q_values.shape)\n",
    "            action = torch.argmax(q_values, dim=1).item()\n",
    "        \n",
    "        #Take the action in the environment and get the next state, reward, terminated, truncated and info\n",
    "        new_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        #Update the total reward and current episode steps\n",
    "        self.total_reward += reward\n",
    "        self.current_episode_steps += 1\n",
    "        \n",
    "        #Create the experience tuple and append it to the experience replay buffer\n",
    "        exp = Experience(self.current_state, action, reward, terminated, truncated, new_state)\n",
    "        self.exp_replay_buffer.append(exp)\n",
    "        \n",
    "        #Update the current state to the new state\n",
    "        self.current_state = new_state\n",
    "        \n",
    "        #Check if the episode is done and if so, update the done_reward and reset the environment\n",
    "        if terminated or truncated:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "            \n",
    "        #return the reward when the episode is done    \n",
    "        return done_reward\n",
    "    \n",
    "    \n",
    "#Test the agent\n",
    "print(Agent(make_env(), ExperienceReplay(10000)).step(check_net, check_strat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the training loop that will train the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      ">>>Training started at: 2023-12-27 10:24:00.028768\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "%load_ext tensorboard\n",
    "print(\">>>Training started at:\", datetime.datetime.now())\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, filename=\"model.pth\"):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ye3qqpn1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Best Mean Reward</td><td>▁</td></tr><tr><td>Episode</td><td>▁</td></tr><tr><td>Epsilon</td><td>▁</td></tr><tr><td>Frame</td><td>▁</td></tr><tr><td>Mean Reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Mean Reward</td><td>-21.0</td></tr><tr><td>Episode</td><td>1</td></tr><tr><td>Epsilon</td><td>0.99078</td></tr><tr><td>Frame</td><td>965</td></tr><tr><td>Mean Reward</td><td>-21.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">colorful-pond-21</strong> at: <a href='https://wandb.ai/neildlf/DQN_RIGHT_PONG/runs/ye3qqpn1' target=\"_blank\">https://wandb.ai/neildlf/DQN_RIGHT_PONG/runs/ye3qqpn1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231227_102247-ye3qqpn1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ye3qqpn1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ndelafuente/RL_PONG/PONG_RL/right_players/right_DQN/wandb/run-20231227_102400-2aecye9l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/neildlf/DQN_RIGHT_PONG/runs/2aecye9l' target=\"_blank\">absurd-bee-22</a></strong> to <a href='https://wandb.ai/neildlf/DQN_RIGHT_PONG' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/neildlf/DQN_RIGHT_PONG' target=\"_blank\">https://wandb.ai/neildlf/DQN_RIGHT_PONG</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/neildlf/DQN_RIGHT_PONG/runs/2aecye9l' target=\"_blank\">https://wandb.ai/neildlf/DQN_RIGHT_PONG/runs/2aecye9l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Training for parameters: 0.0001 0.99 32 10000 100 100000 1.0 0.02 started at: 2023-12-27 10:24:09.884138\n",
      "Episode:1 | Frame:826 | Total games:1 | Mean reward: -21.000 | epsilon used: 0.992\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:2 | Frame:1590 | Total games:2 | Mean reward: -21.000 | epsilon used: 0.985\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:3 | Frame:2354 | Total games:3 | Mean reward: -21.000 | epsilon used: 0.977\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:4 | Frame:3178 | Total games:4 | Mean reward: -21.000 | epsilon used: 0.969\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:5 | Frame:4090 | Total games:5 | Mean reward: -21.000 | epsilon used: 0.961\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:6 | Frame:4914 | Total games:6 | Mean reward: -21.000 | epsilon used: 0.953\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:7 | Frame:5861 | Total games:7 | Mean reward: -21.000 | epsilon used: 0.944\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:8 | Frame:6929 | Total games:8 | Mean reward: -21.000 | epsilon used: 0.934\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:9 | Frame:7741 | Total games:9 | Mean reward: -21.000 | epsilon used: 0.927\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:10 | Frame:8613 | Total games:10 | Mean reward: -21.000 | epsilon used: 0.919\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:11 | Frame:9589 | Total games:11 | Mean reward: -20.909 | epsilon used: 0.910\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:12 | Frame:10415 | Total games:12 | Mean reward: -20.917 | epsilon used: 0.903\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:13 | Frame:11241 | Total games:13 | Mean reward: -20.923 | epsilon used: 0.896\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:14 | Frame:12065 | Total games:14 | Mean reward: -20.929 | epsilon used: 0.889\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:15 | Frame:13269 | Total games:15 | Mean reward: -20.867 | epsilon used: 0.878\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:16 | Frame:14250 | Total games:16 | Mean reward: -20.812 | epsilon used: 0.870\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:17 | Frame:15436 | Total games:17 | Mean reward: -20.706 | epsilon used: 0.860\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:18 | Frame:16751 | Total games:18 | Mean reward: -20.556 | epsilon used: 0.849\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:19 | Frame:18047 | Total games:19 | Mean reward: -20.579 | epsilon used: 0.838\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:20 | Frame:19066 | Total games:20 | Mean reward: -20.600 | epsilon used: 0.830\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:21 | Frame:20833 | Total games:21 | Mean reward: -20.524 | epsilon used: 0.816\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:22 | Frame:22362 | Total games:22 | Mean reward: -20.500 | epsilon used: 0.804\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:23 | Frame:23550 | Total games:23 | Mean reward: -20.478 | epsilon used: 0.794\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:24 | Frame:25224 | Total games:24 | Mean reward: -20.417 | epsilon used: 0.782\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:25 | Frame:26969 | Total games:25 | Mean reward: -20.280 | epsilon used: 0.768\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:26 | Frame:28755 | Total games:26 | Mean reward: -20.308 | epsilon used: 0.755\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:27 | Frame:30535 | Total games:27 | Mean reward: -20.259 | epsilon used: 0.742\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:28 | Frame:32664 | Total games:28 | Mean reward: -20.179 | epsilon used: 0.727\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:29 | Frame:34700 | Total games:29 | Mean reward: -20.138 | epsilon used: 0.713\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:30 | Frame:36159 | Total games:30 | Mean reward: -20.100 | epsilon used: 0.703\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:31 | Frame:37401 | Total games:31 | Mean reward: -20.129 | epsilon used: 0.694\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:32 | Frame:39198 | Total games:32 | Mean reward: -20.062 | epsilon used: 0.682\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:33 | Frame:40815 | Total games:33 | Mean reward: -20.061 | epsilon used: 0.672\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:34 | Frame:42878 | Total games:34 | Mean reward: -19.971 | epsilon used: 0.658\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:35 | Frame:44766 | Total games:35 | Mean reward: -19.857 | epsilon used: 0.646\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:36 | Frame:46732 | Total games:36 | Mean reward: -19.778 | epsilon used: 0.634\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:37 | Frame:48590 | Total games:37 | Mean reward: -19.703 | epsilon used: 0.623\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:38 | Frame:50202 | Total games:38 | Mean reward: -19.632 | epsilon used: 0.613\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:39 | Frame:52360 | Total games:39 | Mean reward: -19.590 | epsilon used: 0.601\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:40 | Frame:54087 | Total games:40 | Mean reward: -19.600 | epsilon used: 0.591\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:41 | Frame:55672 | Total games:41 | Mean reward: -19.561 | epsilon used: 0.582\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:42 | Frame:57404 | Total games:42 | Mean reward: -19.548 | epsilon used: 0.572\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:43 | Frame:59309 | Total games:43 | Mean reward: -19.488 | epsilon used: 0.562\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:44 | Frame:61056 | Total games:44 | Mean reward: -19.432 | epsilon used: 0.552\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:45 | Frame:63124 | Total games:45 | Mean reward: -19.400 | epsilon used: 0.541\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:46 | Frame:65361 | Total games:46 | Mean reward: -19.304 | epsilon used: 0.530\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:47 | Frame:67344 | Total games:47 | Mean reward: -19.298 | epsilon used: 0.520\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:48 | Frame:69639 | Total games:48 | Mean reward: -19.229 | epsilon used: 0.508\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:49 | Frame:71576 | Total games:49 | Mean reward: -19.184 | epsilon used: 0.499\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:50 | Frame:73473 | Total games:50 | Mean reward: -19.180 | epsilon used: 0.490\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:51 | Frame:75398 | Total games:51 | Mean reward: -19.137 | epsilon used: 0.481\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:52 | Frame:77362 | Total games:52 | Mean reward: -19.135 | epsilon used: 0.472\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:53 | Frame:79328 | Total games:53 | Mean reward: -19.113 | epsilon used: 0.463\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:54 | Frame:81843 | Total games:54 | Mean reward: -19.074 | epsilon used: 0.452\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:55 | Frame:84357 | Total games:55 | Mean reward: -19.036 | epsilon used: 0.442\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:56 | Frame:86145 | Total games:56 | Mean reward: -19.054 | epsilon used: 0.434\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:57 | Frame:88248 | Total games:57 | Mean reward: -19.035 | epsilon used: 0.425\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:58 | Frame:90392 | Total games:58 | Mean reward: -19.000 | epsilon used: 0.417\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:59 | Frame:93018 | Total games:59 | Mean reward: -18.949 | epsilon used: 0.407\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:60 | Frame:95232 | Total games:60 | Mean reward: -18.900 | epsilon used: 0.398\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:61 | Frame:97687 | Total games:61 | Mean reward: -18.820 | epsilon used: 0.389\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:62 | Frame:99733 | Total games:62 | Mean reward: -18.823 | epsilon used: 0.381\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:63 | Frame:102093 | Total games:63 | Mean reward: -18.794 | epsilon used: 0.373\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:64 | Frame:104345 | Total games:64 | Mean reward: -18.734 | epsilon used: 0.365\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:65 | Frame:106894 | Total games:65 | Mean reward: -18.646 | epsilon used: 0.357\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:66 | Frame:109343 | Total games:66 | Mean reward: -18.652 | epsilon used: 0.348\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:67 | Frame:111976 | Total games:67 | Mean reward: -18.627 | epsilon used: 0.340\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:68 | Frame:113993 | Total games:68 | Mean reward: -18.618 | epsilon used: 0.333\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:69 | Frame:116567 | Total games:69 | Mean reward: -18.580 | epsilon used: 0.325\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:70 | Frame:118739 | Total games:70 | Mean reward: -18.586 | epsilon used: 0.319\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:71 | Frame:121327 | Total games:71 | Mean reward: -18.521 | epsilon used: 0.311\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:72 | Frame:123579 | Total games:72 | Mean reward: -18.514 | epsilon used: 0.305\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:73 | Frame:126044 | Total games:73 | Mean reward: -18.466 | epsilon used: 0.298\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:74 | Frame:128555 | Total games:74 | Mean reward: -18.459 | epsilon used: 0.291\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:75 | Frame:130993 | Total games:75 | Mean reward: -18.400 | epsilon used: 0.284\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:76 | Frame:133063 | Total games:76 | Mean reward: -18.382 | epsilon used: 0.279\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:77 | Frame:135870 | Total games:77 | Mean reward: -18.312 | epsilon used: 0.272\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:78 | Frame:138449 | Total games:78 | Mean reward: -18.256 | epsilon used: 0.265\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:79 | Frame:140652 | Total games:79 | Mean reward: -18.228 | epsilon used: 0.260\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:80 | Frame:142870 | Total games:80 | Mean reward: -18.225 | epsilon used: 0.255\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:81 | Frame:145184 | Total games:81 | Mean reward: -18.210 | epsilon used: 0.249\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:82 | Frame:147262 | Total games:82 | Mean reward: -18.183 | epsilon used: 0.245\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:83 | Frame:149186 | Total games:83 | Mean reward: -18.205 | epsilon used: 0.240\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:84 | Frame:151306 | Total games:84 | Mean reward: -18.214 | epsilon used: 0.236\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:85 | Frame:153721 | Total games:85 | Mean reward: -18.176 | epsilon used: 0.231\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:86 | Frame:156320 | Total games:86 | Mean reward: -18.174 | epsilon used: 0.225\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:87 | Frame:159125 | Total games:87 | Mean reward: -18.149 | epsilon used: 0.220\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:88 | Frame:161908 | Total games:88 | Mean reward: -18.057 | epsilon used: 0.214\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:89 | Frame:164286 | Total games:89 | Mean reward: -18.034 | epsilon used: 0.210\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:90 | Frame:166716 | Total games:90 | Mean reward: -18.033 | epsilon used: 0.205\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:91 | Frame:168682 | Total games:91 | Mean reward: -18.055 | epsilon used: 0.201\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:92 | Frame:171212 | Total games:92 | Mean reward: -18.000 | epsilon used: 0.197\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:93 | Frame:173442 | Total games:93 | Mean reward: -17.989 | epsilon used: 0.193\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:94 | Frame:176755 | Total games:94 | Mean reward: -17.926 | epsilon used: 0.187\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:95 | Frame:178491 | Total games:95 | Mean reward: -17.937 | epsilon used: 0.184\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:96 | Frame:181345 | Total games:96 | Mean reward: -17.917 | epsilon used: 0.180\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:97 | Frame:183824 | Total games:97 | Mean reward: -17.887 | epsilon used: 0.176\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:98 | Frame:186075 | Total games:98 | Mean reward: -17.888 | epsilon used: 0.172\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:99 | Frame:188249 | Total games:99 | Mean reward: -17.889 | epsilon used: 0.169\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:100 | Frame:190623 | Total games:100 | Mean reward: -17.880 | epsilon used: 0.166\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:101 | Frame:193425 | Total games:101 | Mean reward: -17.800 | epsilon used: 0.162\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:102 | Frame:195521 | Total games:102 | Mean reward: -17.780 | epsilon used: 0.159\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:103 | Frame:198010 | Total games:103 | Mean reward: -17.740 | epsilon used: 0.155\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:104 | Frame:200304 | Total games:104 | Mean reward: -17.710 | epsilon used: 0.152\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:105 | Frame:202415 | Total games:105 | Mean reward: -17.680 | epsilon used: 0.149\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:106 | Frame:205057 | Total games:106 | Mean reward: -17.630 | epsilon used: 0.146\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:107 | Frame:207406 | Total games:107 | Mean reward: -17.610 | epsilon used: 0.143\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:108 | Frame:209842 | Total games:108 | Mean reward: -17.550 | epsilon used: 0.140\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:109 | Frame:212015 | Total games:109 | Mean reward: -17.540 | epsilon used: 0.138\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:110 | Frame:214757 | Total games:110 | Mean reward: -17.490 | epsilon used: 0.134\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:111 | Frame:217320 | Total games:111 | Mean reward: -17.440 | epsilon used: 0.132\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:112 | Frame:220049 | Total games:112 | Mean reward: -17.360 | epsilon used: 0.129\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:113 | Frame:222258 | Total games:113 | Mean reward: -17.330 | epsilon used: 0.126\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:114 | Frame:225099 | Total games:114 | Mean reward: -17.220 | epsilon used: 0.123\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:115 | Frame:227149 | Total games:115 | Mean reward: -17.200 | epsilon used: 0.121\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:116 | Frame:229676 | Total games:116 | Mean reward: -17.130 | epsilon used: 0.119\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:117 | Frame:231784 | Total games:117 | Mean reward: -17.100 | epsilon used: 0.117\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:118 | Frame:233585 | Total games:118 | Mean reward: -17.120 | epsilon used: 0.115\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:119 | Frame:235878 | Total games:119 | Mean reward: -17.080 | epsilon used: 0.113\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n",
      "Episode:120 | Frame:238080 | Total games:120 | Mean reward: -17.030 | epsilon used: 0.111\n",
      "lr=0.0001 | gamma=0.99 | batch_size=32 | target_update=100 | epsilon_decay=100000 | epsilon_start=1.0 | epsilon_end=0.02 | buffer_size=10000\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "'''\n",
    "We'll define the hyperparameters that will be used during the training phase. \n",
    "Our goal is to do a grid search over the hyperparameters to find the best combination \n",
    "of hyperparameters that will give us the best performance, so we'll define lists that\n",
    "will contain all the hyperparameters that we want to search over.\n",
    "'''\n",
    "#Hyperparameters\n",
    "lrs = [0.0001, 0.00025, 0.0005, 0.001, 0.0025, 0.005]\n",
    "gammas = [0.99, 0.95, 0.9, 0.85, 0.8]\n",
    "batch_sizes = [32, 64, 128, 256, 512]\n",
    "buffer_sizes = [10000, 20000, 50000, 100000, 200000]\n",
    "target_updates = [100, 200, 500, 1000, 2000]\n",
    "epsilon_decays = [100000, 200000, 500000, 1000000, 2000000]\n",
    "epsilon_starts = [1.0, 0.9, 0.8, 0.7, 0.6]\n",
    "epsilon_ends = [0.02, 0.04, 0.06, 0.08, 0.1]\n",
    "env_name = \"ALE/Pong-v5\"\n",
    "num_episodes = 10000000\n",
    "max_steps_per_episode = 10000000\n",
    "solved_reward = 19\n",
    "min_episodes = 100\n",
    "max_no_improvement = 100\n",
    "\n",
    "\n",
    "#Remember we want to do a grid search over the hyperparameters, so we will have to loop over all the hyperparameters\n",
    "\n",
    "#Grid search over the hyperparameters\n",
    "results = [] \n",
    "for lr in lrs:\n",
    "    for gamma in gammas:\n",
    "        for batch_size in batch_sizes:\n",
    "            for buffer_size in buffer_sizes:\n",
    "                for target_update in target_updates:\n",
    "                    for epsilon_decay in epsilon_decays:\n",
    "                        for epsilon_start in epsilon_starts:\n",
    "                            for epsilon_end in epsilon_ends:\n",
    "                                #Initialize wandb\n",
    "                                wandb.init(project=\"DQN_RIGHT_PONG\", entity=\"neildlf\", config={\n",
    "                                    'lr': lr,\n",
    "                                    'gamma': gamma,\n",
    "                                    'batch_size': batch_size,\n",
    "                                    'buffer_size': buffer_size,\n",
    "                                    'target_update': target_update,\n",
    "                                    'epsilon_decay': epsilon_decay,\n",
    "                                    'epsilon_start': epsilon_start,\n",
    "                                    'epsilon_end': epsilon_end,\n",
    "                                })\n",
    "                                \n",
    "                                print(\">>>Training for parameters:\", lr, gamma, batch_size, buffer_size, target_update, epsilon_decay, epsilon_start, epsilon_end, \"started at:\", datetime.datetime.now(), \"on device:\", device)\n",
    "                                #Create the environment\n",
    "                                env = make_env()\n",
    "                                #Create the experience replay buffer\n",
    "                                exp_replay_buffer = ExperienceReplay(capacity=buffer_size)\n",
    "                                #Create the agent\n",
    "                                agent = Agent(env, exp_replay_buffer)\n",
    "                                #Create the DQN\n",
    "                                dqn = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "                                #Create the target DQN\n",
    "                                target_dqn = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "                                #Set the target DQN's weights to be the same as the DQN\n",
    "                                target_dqn.load_state_dict(dqn.state_dict())\n",
    "                                #Set the target DQN to evaluation mode\n",
    "                                target_dqn.eval()\n",
    "                                #Create the optimizer\n",
    "                                optimizer = optim.Adam(dqn.parameters(), lr=lr)\n",
    "                                #Create the strategy\n",
    "                                strategy = EpsilonGreedyStrategy(epsilon_start, epsilon_end, epsilon_decay)\n",
    "                                #Set the frame number to 0\n",
    "                                frame_number = 0\n",
    "                                #set the episode number to 0\n",
    "                                episode_number = 0\n",
    "                                #Set the total reward list to empty\n",
    "                                total_reward_list = []\n",
    "                                \n",
    "                                #Set the best mean reward to -infinity initially\n",
    "                                best_mean_reward = -float('inf')\n",
    "                                #Set the no improvement count to 0\n",
    "                                no_improvement_count = 0\n",
    "                                #Loop over the episodes\n",
    "                                for _ in range(num_episodes):\n",
    "                                    frame_number += 1\n",
    "                                    epsilon = strategy.get_exploration_rate(frame_number)\n",
    "                                    \n",
    "                                    #Take a step in the environment\n",
    "                                    reward = agent.step(dqn, strategy)\n",
    "                                    \n",
    "                                    #If the reward is not None, then the episode is done\n",
    "                                    if reward is not None:\n",
    "                                        episode_number += 1\n",
    "                                        #Append the total reward to the total reward list\n",
    "                                        total_reward_list.append(reward)\n",
    "                                        #Get the mean of the total reward list\n",
    "                                        mean_reward = np.mean(total_reward_list[-100:])\n",
    "                                        #Print the episode number, frame number, reward and mean reward and epsilon\n",
    "                                        print(f\"Episode:{episode_number} | Frame:{frame_number} | Total games:{len(total_reward_list)}  | Episode reward: {reward:.3f} | Mean reward: {mean_reward:.3f} | epsilon used: {epsilon:.3f}\")\n",
    "                                        #Print all the hyperparameters used in this episode\n",
    "                                        print(f\"lr={lr} | gamma={gamma} | batch_size={batch_size} | target_update={target_update} | epsilon_decay={epsilon_decay} | epsilon_start={epsilon_start} | epsilon_end={epsilon_end} | buffer_size={buffer_size}\")\n",
    "                                        \n",
    "                                        #Add the mean reward, episode number, frame number and epsilon to the wandb logs\n",
    "                                        wandb.log({'Episode reward': reward, 'Mean Reward': mean_reward, 'Episode': episode_number, 'Frame': frame_number, 'Epsilon': epsilon})\n",
    "\n",
    "                                        #If the mean reward is greater than \"solved_reward\", then we have solved the environment\n",
    "                                        if mean_reward > solved_reward:\n",
    "                                            print(\"Solved in\", frame_number, \"frames and\", len(total_reward_list), \"games!\")\n",
    "                                            #break\n",
    "                                        \n",
    "                                        if mean_reward > best_mean_reward:\n",
    "                                            best_mean_reward = mean_reward\n",
    "                                            save_model(dqn, optimizer, filename=f\"model_{lr}_{gamma}_{batch_size}_{buffer_size}_{target_update}_{epsilon_decay}_{epsilon_start}_{epsilon_end}.pth\")\n",
    "                                            wandb.log({'Best Mean Reward': best_mean_reward})\n",
    "                                        else:\n",
    "                                            no_improvement_count += 1\n",
    "                                        \n",
    "                                        #enforce early stopping if the model has converged\n",
    "                                        if episode_number >= min_episodes and no_improvement_count >= max_no_improvement:\n",
    "                                            print(\"Stopping training as the model has converged!\")\n",
    "                                            break\n",
    "                                        \n",
    "                                    #Check if the replay buffer has enough experience to sample a batch    \n",
    "                                    if len(exp_replay_buffer) < batch_size:\n",
    "                                        continue\n",
    "                                    \n",
    "                                    #If the replay buffer has enough experience to sample a batch, then sample a batch\n",
    "                                    batch = exp_replay_buffer.sample(batch_size)\n",
    "                                    \n",
    "                                    #Get the states, actions, rewards, terminated, truncated, new_states from the batch\n",
    "                                    states_, actions_, rewards_, terminated_, truncated_, new_states_ = batch\n",
    "                                    \n",
    "                                    #Turn the states, actions, rewards, terminated, truncated, new_states into tensors and send them to the device\n",
    "                                    states = torch.tensor(np.array(states_).transpose(0, 3, 1, 2), dtype=torch.float32).to(device)\n",
    "                                    actions = torch.tensor(actions_).to(device)\n",
    "                                    rewards = torch.tensor(rewards_).to(device)\n",
    "                                    terminateds = torch.tensor(terminated_).to(device)\n",
    "                                    truncateds = torch.tensor(truncated_).to(device)\n",
    "                                    #as bitwise cuda is not implemented for floats, we will use the logical and operator to combine the terminateds and truncateds\n",
    "                                    dones = torch.logical_or(terminateds, truncateds).to(device)\n",
    "                                    new_states = torch.tensor(np.array(new_states_).transpose(0, 3, 1, 2), dtype=torch.float32).to(device)\n",
    "                                    \n",
    "                                    \n",
    "                                    #Get the q_values from the DQN by passing the states through the DQN\n",
    "                                    Q_values = dqn(states).gather(dim=1, index=actions.unsqueeze(-1)).squeeze(-1) # Use the gather method to get the q_values for the actions taken, and then squeeze the last dimension of the q_values for the loss calculation\n",
    "                                    \n",
    "                                    #Get the q_values for next state from the target DQN by passing the new_states through the target DQN\n",
    "                                    new_state_Q_values = target_dqn(new_states).max(dim=1)[0] # Get the q_values for the new_states from the target DQN and take the max of the q_values\n",
    "                                    new_state_Q_values[dones] = 0 # If the episode is terminated or truncated, then set the new_state_q_values to 0\n",
    "                                    new_state_Q_values = new_state_Q_values.detach() # Detach the new_state_q_values from the computational graph so that the gradients are not calculated for the new_state_q_values\n",
    "                                    \n",
    "                                    #Compute the expected q_values using the bellman equation\n",
    "                                    expected_Q_values = rewards + gamma * new_state_Q_values\n",
    "                                    \n",
    "                                    #Compute the loss between the Q_values and the expected_Q_values\n",
    "                                    loss = F.smooth_l1_loss(Q_values, expected_Q_values) # Use the smooth_l1_loss function to compute the loss, I use this loss function because it is less sensitive to outliers than the mse loss function\n",
    "                                    \n",
    "                                    #Zero the gradients\n",
    "                                    optimizer.zero_grad()\n",
    "                                    #Compute the gradients\n",
    "                                    loss.backward()\n",
    "                                    #Clip the gradients\n",
    "                                    for param in dqn.parameters():\n",
    "                                        param.grad.clamp_(-1, 1) # Use the clamp_ method to clip the gradients between -1 and 1 to avoid exploding gradients\n",
    "                                    #Update the weights of the DQN\n",
    "                                    optimizer.step()\n",
    "                                    \n",
    "                                    #Check if the frame number is a multiple of the target_update and if so, update the target DQN's weights to be the same as the DQN\n",
    "                                    if frame_number % target_update == 0:\n",
    "                                        target_dqn.load_state_dict(dqn.state_dict())\n",
    "                                \n",
    "                                results.append({\n",
    "                                    'lr': lr,\n",
    "                                    'gamma': gamma,\n",
    "                                    'batch_size': batch_size,\n",
    "                                    'buffer_size': buffer_size,\n",
    "                                    'target_update': target_update,\n",
    "                                    'epsilon_decay': epsilon_decay,\n",
    "                                    'epsilon_start': epsilon_start,\n",
    "                                    'epsilon_end': epsilon_end,\n",
    "                                    'best_mean_reward': best_mean_reward,\n",
    "                                    'episodes_to_solve': episode_number,\n",
    "                                })\n",
    "                                \n",
    "                                #Close the writer\n",
    "                                wandb.finish()\n",
    "                                \n",
    "                                    \n",
    "# Save results to a file\n",
    "with open(\"training_results.txt\", \"w\") as file:\n",
    "    for result in results:\n",
    "        file.write(str(result) + \"\\n\")                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "def train_hyperparameters(hyperparams):\n",
    "    lr, gamma, batch_size, buffer_size, target_update, epsilon_decay, epsilon_start, epsilon_end = hyperparams\n",
    "    device_id = multiprocessing.current_process()._identity[0] % torch.cuda.device_count()\n",
    "    device = torch.device(f'cuda:{device_id}' if torch.cuda.is_available() else 'cpu')\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(device_id)\n",
    "\n",
    "    wandb.init(project=\"DQN_RIGHT_PONG\", entity=\"neildlf\", config={\n",
    "        'lr': lr,\n",
    "        'gamma': gamma,\n",
    "        'batch_size': batch_size,\n",
    "        'buffer_size': buffer_size,\n",
    "        'target_update': target_update,\n",
    "        'epsilon_decay': epsilon_decay,\n",
    "        'epsilon_start': epsilon_start,\n",
    "        'epsilon_end': epsilon_end,\n",
    "    })\n",
    "\n",
    "    print(f\"Training on GPU {device_id} for parameters: {lr}, {gamma}, {batch_size}, {buffer_size}, {target_update}, {epsilon_decay}, {epsilon_start}, {epsilon_end}\")\n",
    "\n",
    "    env = make_env()\n",
    "    exp_replay_buffer = ExperienceReplay(capacity=buffer_size)\n",
    "    agent = Agent(env, exp_replay_buffer)\n",
    "    dqn = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    target_dqn = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    target_dqn.load_state_dict(dqn.state_dict())\n",
    "    target_dqn.eval()\n",
    "    optimizer = optim.Adam(dqn.parameters(), lr=lr)\n",
    "    strategy = EpsilonGreedyStrategy(epsilon_start, epsilon_end, epsilon_decay)\n",
    "\n",
    "    frame_number = 0\n",
    "    episode_number = 0\n",
    "    best_mean_reward = -float('inf')\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        frame_number += 1\n",
    "        epsilon = strategy.get_exploration_rate(frame_number)\n",
    "        \n",
    "        reward = agent.step(dqn, strategy, device)\n",
    "        if reward is not None:\n",
    "            episode_number += 1\n",
    "            total_reward_list.append(reward)\n",
    "            mean_reward = np.mean(total_reward_list[-100:])\n",
    "            wandb.log({'Mean Reward': mean_reward, 'Episode': episode_number, 'Frame': frame_number, 'Epsilon': epsilon})\n",
    "\n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                save_model(dqn, optimizer, filename=f\"model_{lr}_{gamma}_{batch_size}_{buffer_size}_{target_update}_{epsilon_decay}_{epsilon_start}_{epsilon_end}.pth\")\n",
    "                wandb.log({'Best Mean Reward': best_mean_reward})\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "\n",
    "            if episode_number >= min_episodes and no_improvement_count >= max_no_improvement:\n",
    "                print(\"Stopping training due to no improvement.\")\n",
    "                break\n",
    "\n",
    "        if len(exp_replay_buffer) < batch_size:\n",
    "            continue\n",
    "\n",
    "        states_, actions_, rewards_, terminated_, truncated_, new_states_ = exp_replay_buffer.sample(batch_size)\n",
    "        states = torch.tensor(states_, dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(actions_).to(device)\n",
    "        rewards = torch.tensor(rewards_).to(device)\n",
    "        terminateds = torch.tensor(terminated_).to(device)\n",
    "        truncateds = torch.tensor(truncated_).to(device)\n",
    "        dones = torch.logical_or(terminateds, truncateds).to(device)\n",
    "        new_states = torch.tensor(new_states_, dtype=torch.float32).to(device)\n",
    "\n",
    "        Q_values = dqn(states).gather(dim=1, index=actions.unsqueeze(-1)).squeeze(-1)\n",
    "        new_state_Q_values = target_dqn(new_states).max(dim=1)[0]\n",
    "        new_state_Q_values[dones] = 0\n",
    "        new_state_Q_values = new_state_Q_values.detach()\n",
    "        expected_Q_values = rewards + gamma * new_state_Q_values\n",
    "        loss = F.smooth_l1_loss(Q_values, expected_Q_values)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in dqn.parameters():\n",
    "            param.grad.clamp_(-1, 1)\n",
    "        optimizer.step()\n",
    "\n",
    "        if frame_number % target_update == 0:\n",
    "            target_dqn.load_state_dict(dqn.state_dict())\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Hyperparameters and constants\n",
    "num_episodes = 10000000\n",
    "max_steps_per_episode = 10000000\n",
    "solved_reward = 19\n",
    "min_episodes = 100\n",
    "max_no_improvement = 100\n",
    "lrs = [0.0001, 0.00025, 0.0005, 0.001, 0.0025, 0.005]\n",
    "gammas = [0.99, 0.95, 0.9, 0.85, 0.8]\n",
    "batch_sizes = [32, 64, 128, 256, 512]\n",
    "buffer_sizes = [10000, 20000, 50000, 100000, 200000]\n",
    "target_updates = [100, 200, 500, 1000, 2000]\n",
    "epsilon_decays = [100000, 200000, 500000, 1000000, 2000000]\n",
    "epsilon_starts = [1.0, 0.9, 0.8, 0.7, 0.6]\n",
    "epsilon_ends = [0.02, 0.04, 0.06, 0.08, 0.1]\n",
    "env_name = \"ALE/Pong-v5\"\n",
    "\n",
    "\n",
    "\n",
    "all_hyperparams = [(lr, gamma, batch_size, buffer_size, target_update, epsilon_decay, epsilon_start, epsilon_end)\n",
    "                   for lr in lrs for gamma in gammas for batch_size in batch_sizes\n",
    "                   for buffer_size in buffer_sizes for target_update in target_updates\n",
    "                   for epsilon_decay in epsilon_decays for epsilon_start in epsilon_starts\n",
    "                   for epsilon_end in epsilon_ends]\n",
    "\n",
    "pool = multiprocessing.Pool(processes=6)  # Adjust based on your GPU count\n",
    "pool.map(train_hyperparameters, all_hyperparams)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Save results to a file\n",
    "with open(\"training_results.txt\", \"w\") as file:\n",
    "    for result in results:\n",
    "        file.write(str(result) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
